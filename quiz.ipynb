{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrical Grid Stability Simulated Data Data Set\n",
    "\n",
    "\n",
    "Stability of the Grid System\n",
    "\n",
    "Electrical grids require a balance between electricity supply and demand in order to be stable. Conventional systems achieve this balance through demand-driven electricity production. For future grids with a high share of inflexible (i.e., renewable) energy sources, the concept of demand response is a promising solution. This implies changes in electricity consumption in relation to electricity price changes. \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Weâ€™ll build a binary classification model to predict if a grid is stable or unstable using the UCI Electrical Grid Stability Simulated dataset.\n",
    "\n",
    "#### Predictive features:\n",
    "\n",
    "'tau1' to 'tau4': the reaction time of each network participant, a real value within the range 0.5 to 10 ('tau1' corresponds to the supplier node, 'tau2' to 'tau4' to the consumer nodes);\n",
    "\n",
    "'p1' to 'p4': nominal power produced (positive) or consumed (negative) by each network participant, a real value within the range -2.0 to -0.5 for consumers ('p2' to 'p4'). As the total power consumed equals the total power generated, p1 (supplier node) = - (p2 + p3 + p4);\n",
    "\n",
    "'g1' to 'g4': price elasticity coefficient for each network participant, a real value within the range 0.05 to 1.00 ('g1' corresponds to the supplier node, 'g2' to 'g4' to the consumer nodes; 'g' stands for 'gamma');\n",
    "\n",
    "\n",
    "#### Dependent variables:\n",
    "\n",
    "'stab': the maximum real part of the characteristic differential equation root (if positive, the system is linearly unstable; if negative, linearly stable);\n",
    "'stabf': a categorical (binary) label ('stable' or 'unstable')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the  required libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the datasets\n",
    "df = pd.read_csv('Data_for_UCI_named.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>-0.005957</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.999209</td>\n",
       "      <td>9.109247</td>\n",
       "      <td>3.784066</td>\n",
       "      <td>4.267788</td>\n",
       "      <td>4.429669</td>\n",
       "      <td>-1.857139</td>\n",
       "      <td>-0.670397</td>\n",
       "      <td>-1.902133</td>\n",
       "      <td>0.261793</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.542884</td>\n",
       "      <td>0.469931</td>\n",
       "      <td>-0.017385</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.710166</td>\n",
       "      <td>3.765204</td>\n",
       "      <td>6.929314</td>\n",
       "      <td>8.818562</td>\n",
       "      <td>2.397419</td>\n",
       "      <td>-0.614590</td>\n",
       "      <td>-1.208826</td>\n",
       "      <td>-0.574004</td>\n",
       "      <td>0.177890</td>\n",
       "      <td>0.397977</td>\n",
       "      <td>0.402046</td>\n",
       "      <td>0.376630</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.953512</td>\n",
       "      <td>1.379125</td>\n",
       "      <td>5.719400</td>\n",
       "      <td>7.870307</td>\n",
       "      <td>3.224495</td>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-1.186517</td>\n",
       "      <td>-1.288980</td>\n",
       "      <td>0.371385</td>\n",
       "      <td>0.633204</td>\n",
       "      <td>0.732741</td>\n",
       "      <td>0.380544</td>\n",
       "      <td>0.016634</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.689852</td>\n",
       "      <td>4.007747</td>\n",
       "      <td>1.478573</td>\n",
       "      <td>3.733787</td>\n",
       "      <td>4.041300</td>\n",
       "      <td>-1.410344</td>\n",
       "      <td>-1.238204</td>\n",
       "      <td>-1.392751</td>\n",
       "      <td>0.269708</td>\n",
       "      <td>0.250364</td>\n",
       "      <td>0.164941</td>\n",
       "      <td>0.482439</td>\n",
       "      <td>-0.038677</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.841496</td>\n",
       "      <td>1.413822</td>\n",
       "      <td>9.769856</td>\n",
       "      <td>7.641616</td>\n",
       "      <td>4.727595</td>\n",
       "      <td>-1.991363</td>\n",
       "      <td>-0.857637</td>\n",
       "      <td>-1.878594</td>\n",
       "      <td>0.376356</td>\n",
       "      <td>0.544415</td>\n",
       "      <td>0.792039</td>\n",
       "      <td>0.116263</td>\n",
       "      <td>0.012383</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "5  6.999209  9.109247  3.784066  4.267788  4.429669 -1.857139 -0.670397   \n",
       "6  6.710166  3.765204  6.929314  8.818562  2.397419 -0.614590 -1.208826   \n",
       "7  6.953512  1.379125  5.719400  7.870307  3.224495 -0.748998 -1.186517   \n",
       "8  4.689852  4.007747  1.478573  3.733787  4.041300 -1.410344 -1.238204   \n",
       "9  9.841496  1.413822  9.769856  7.641616  4.727595 -1.991363 -0.857637   \n",
       "\n",
       "         p4        g1        g2        g3        g4      stab     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860  unstable  \n",
       "5 -1.902133  0.261793  0.077930  0.542884  0.469931 -0.017385    stable  \n",
       "6 -0.574004  0.177890  0.397977  0.402046  0.376630  0.005954  unstable  \n",
       "7 -1.288980  0.371385  0.633204  0.732741  0.380544  0.016634  unstable  \n",
       "8 -1.392751  0.269708  0.250364  0.164941  0.482439 -0.038677    stable  \n",
       "9 -1.878594  0.376356  0.544415  0.792039  0.116263  0.012383  unstable  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an overview of the 10 first datasets\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape is:  (10000, 14)\n"
     ]
    }
   ],
   "source": [
    "# checking the dimensions of the data frame\n",
    "print(\"The Shape is: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tau1     float64\n",
       "tau2     float64\n",
       "tau3     float64\n",
       "tau4     float64\n",
       "p1       float64\n",
       "p2       float64\n",
       "p3       float64\n",
       "p4       float64\n",
       "g1       float64\n",
       "g2       float64\n",
       "g3       float64\n",
       "g4       float64\n",
       "stab     float64\n",
       "stabf     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.250000</td>\n",
       "      <td>5.250001</td>\n",
       "      <td>5.250004</td>\n",
       "      <td>5.249997</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.015731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.742548</td>\n",
       "      <td>2.742549</td>\n",
       "      <td>2.742549</td>\n",
       "      <td>2.742556</td>\n",
       "      <td>0.752160</td>\n",
       "      <td>0.433035</td>\n",
       "      <td>0.433035</td>\n",
       "      <td>0.433035</td>\n",
       "      <td>0.274256</td>\n",
       "      <td>0.274255</td>\n",
       "      <td>0.274255</td>\n",
       "      <td>0.274255</td>\n",
       "      <td>0.036919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.500793</td>\n",
       "      <td>0.500141</td>\n",
       "      <td>0.500788</td>\n",
       "      <td>0.500473</td>\n",
       "      <td>1.582590</td>\n",
       "      <td>-1.999891</td>\n",
       "      <td>-1.999945</td>\n",
       "      <td>-1.999926</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.050053</td>\n",
       "      <td>0.050054</td>\n",
       "      <td>0.050028</td>\n",
       "      <td>-0.080760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.874892</td>\n",
       "      <td>2.875140</td>\n",
       "      <td>2.875522</td>\n",
       "      <td>2.874950</td>\n",
       "      <td>3.218300</td>\n",
       "      <td>-1.624901</td>\n",
       "      <td>-1.625025</td>\n",
       "      <td>-1.624960</td>\n",
       "      <td>0.287521</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>0.287514</td>\n",
       "      <td>0.287494</td>\n",
       "      <td>-0.015557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.250004</td>\n",
       "      <td>5.249981</td>\n",
       "      <td>5.249979</td>\n",
       "      <td>5.249734</td>\n",
       "      <td>3.751025</td>\n",
       "      <td>-1.249966</td>\n",
       "      <td>-1.249974</td>\n",
       "      <td>-1.250007</td>\n",
       "      <td>0.525009</td>\n",
       "      <td>0.525003</td>\n",
       "      <td>0.525015</td>\n",
       "      <td>0.525002</td>\n",
       "      <td>0.017142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.624690</td>\n",
       "      <td>7.624893</td>\n",
       "      <td>7.624948</td>\n",
       "      <td>7.624838</td>\n",
       "      <td>4.282420</td>\n",
       "      <td>-0.874977</td>\n",
       "      <td>-0.875043</td>\n",
       "      <td>-0.875065</td>\n",
       "      <td>0.762435</td>\n",
       "      <td>0.762490</td>\n",
       "      <td>0.762440</td>\n",
       "      <td>0.762433</td>\n",
       "      <td>0.044878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999469</td>\n",
       "      <td>9.999837</td>\n",
       "      <td>9.999450</td>\n",
       "      <td>9.999443</td>\n",
       "      <td>5.864418</td>\n",
       "      <td>-0.500108</td>\n",
       "      <td>-0.500072</td>\n",
       "      <td>-0.500025</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>0.109403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tau1          tau2          tau3          tau4            p1  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       5.250000      5.250001      5.250004      5.249997      3.750000   \n",
       "std        2.742548      2.742549      2.742549      2.742556      0.752160   \n",
       "min        0.500793      0.500141      0.500788      0.500473      1.582590   \n",
       "25%        2.874892      2.875140      2.875522      2.874950      3.218300   \n",
       "50%        5.250004      5.249981      5.249979      5.249734      3.751025   \n",
       "75%        7.624690      7.624893      7.624948      7.624838      4.282420   \n",
       "max        9.999469      9.999837      9.999450      9.999443      5.864418   \n",
       "\n",
       "                 p2            p3            p4            g1            g2  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -1.250000     -1.250000     -1.250000      0.525000      0.525000   \n",
       "std        0.433035      0.433035      0.433035      0.274256      0.274255   \n",
       "min       -1.999891     -1.999945     -1.999926      0.050009      0.050053   \n",
       "25%       -1.624901     -1.625025     -1.624960      0.287521      0.287552   \n",
       "50%       -1.249966     -1.249974     -1.250007      0.525009      0.525003   \n",
       "75%       -0.874977     -0.875043     -0.875065      0.762435      0.762490   \n",
       "max       -0.500108     -0.500072     -0.500025      0.999937      0.999944   \n",
       "\n",
       "                 g3            g4          stab  \n",
       "count  10000.000000  10000.000000  10000.000000  \n",
       "mean       0.525000      0.525000      0.015731  \n",
       "std        0.274255      0.274255      0.036919  \n",
       "min        0.050054      0.050028     -0.080760  \n",
       "25%        0.287514      0.287494     -0.015557  \n",
       "50%        0.525015      0.525002      0.017142  \n",
       "75%        0.762440      0.762433      0.044878  \n",
       "max        0.999982      0.999930      0.109403  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tau1     0\n",
       "tau2     0\n",
       "tau3     0\n",
       "tau4     0\n",
       "p1       0\n",
       "p2       0\n",
       "p3       0\n",
       "p4       0\n",
       "g1       0\n",
       "g2       0\n",
       "g3       0\n",
       "g4       0\n",
       "stab     0\n",
       "stabf    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stab     \n",
       "-0.080760    1\n",
       " 0.034934    1\n",
       " 0.034883    1\n",
       " 0.034884    1\n",
       " 0.034896    1\n",
       "            ..\n",
       "-0.004154    1\n",
       "-0.004145    1\n",
       "-0.004137    1\n",
       "-0.004133    1\n",
       " 0.109403    1\n",
       "Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of Dependent variable stab\n",
    "df.value_counts([\"stab\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stabf   \n",
       "unstable    6380\n",
       "stable      3620\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of Dependent variable stabf\n",
    "df.value_counts([\"stabf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the direct relationship between 'stab' and 'stabf' ('stabf' = 'stable' if 'stab' <= 0, 'unstable' otherwise),\n",
    "'stab' should be dropped and 'stabf' will remain as the sole dependent variable (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the stab column\n",
    "df.drop('stab', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0  2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1  9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2  8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3  0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4  3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "\n",
       "         p4        g1        g2        g3        g4     stabf  \n",
       "0 -1.723086  0.650456  0.859578  0.887445  0.958034  unstable  \n",
       "1 -1.255012  0.413441  0.862414  0.562139  0.781760    stable  \n",
       "2 -0.920492  0.163041  0.766689  0.839444  0.109853  unstable  \n",
       "3 -0.997374  0.446209  0.976744  0.929381  0.362718  unstable  \n",
       "4 -0.554305  0.797110  0.455450  0.656947  0.820923  unstable  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an overview of the dataset after dropping the stab column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stabf feature has [1 0] unique values\n"
     ]
    }
   ],
   "source": [
    "import sklearn.utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encoding categorical variable\n",
    "df = sklearn.utils.shuffle(df)\n",
    "encoder = LabelEncoder()\n",
    "df['stabf'] = encoder.fit_transform(df['stabf'])\n",
    "print(f\"{'The stabf feature'} has {df['stabf'].unique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns= 'stabf' )\n",
    "y = df[ 'stabf' ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5140\n",
       "0    2860\n",
       "Name: stabf, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spliting the dataset into training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size= 0.2 , random_state= 1 )\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value count operation, it shows that there is imbalance in the class distribution. For this, SMOTE will be used on the training data to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5140\n",
       "1    5140\n",
       "Name: stabf, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state= 1 )\n",
    "x_train_balanced, y_balanced = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "y_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the  SMOTE operation, we can see that there is no imbalance in the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Scaler_x_train_balanced = scaler.fit_transform(x_train_balanced)\n",
    "Scaler_x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19 -What is the accuracy on the test set using the random forest classifier in 4 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rand_fst = RandomForestClassifier(random_state=1)\n",
    "\n",
    "rand_fst.fit(Scaler_x_train_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_fst_y = rand_fst.predict(Scaler_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92071833, 0.914876  , 0.93773942, 0.95861097, 0.95372545])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation and accuracy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(rand_fst, Scaler_x_train_balanced, y_balanced, cv= 5 , scoring= 'f1_macro' )\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer -The accuracy on the test set using the random forest classifier in 4 decimal places is 0.9207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 701,   59],\n",
       "       [  93, 1147]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "# confusion matrix = cnf_mat\n",
    "new_predictions = rand_fst.predict(Scaler_x_test)\n",
    "cnf_mat = confusion_matrix(y_true=y_test, y_pred=new_predictions)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ETC_model = ExtraTreesClassifier(random_state=1)\n",
    "ETC_model.fit(Scaler_x_train_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETC_y = ETC_model.predict(Scaler_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92898772, 0.93821892, 0.9464976 , 0.96249603, 0.96152468])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation and accuracy\n",
    "scores = cross_val_score(ETC_model, Scaler_x_train_balanced, y_balanced, cv= 5 , scoring= 'f1_macro' )\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 685,   75],\n",
       "       [  64, 1176]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "# confusion matrix = cnf_mat\n",
    "new_predictions = ETC_model.predict(Scaler_x_test)\n",
    "cnf_mat = confusion_matrix(y_true=y_test, y_pred=new_predictions)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 -What is the accuracy on the test set using the XGboost classifier. In 4 decimal places?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:42:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=4,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=1,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(random_state=1)\n",
    "xgb_model.fit(Scaler_x_train_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_y = xgb_model.predict(Scaler_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:46:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94357544, 0.94308045, 0.96157543, 0.97469438, 0.9688468 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation and accuracy\n",
    "scores = cross_val_score(xgb_model, Scaler_x_train_balanced, y_balanced, cv= 5 , scoring= 'f1_macro' )\n",
    "scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 715,   45],\n",
       "       [  60, 1180]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "# confusion matrix = cnf_mat\n",
    "new_predictions = xgb_model.predict(Scaler_x_test)\n",
    "cnf_mat = confusion_matrix(y_true=y_test, y_pred=new_predictions)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer -The accuracy on the test set using the XGboost classifier in 4 decimal places is 0.9435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 -What is the accuracy on the test set using the LGBM classifier? In 4 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(random_state=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(random_state=1)\n",
    "lgb_model.fit(Scaler_x_train_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y = lgb_model.predict(Scaler_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93628222, 0.93481991, 0.95525084, 0.9644649 , 0.96592028])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation and accuracy\n",
    "scores = cross_val_score(lgb_model, Scaler_x_train_balanced, y_balanced, cv= 5 , scoring= 'f1_macro' )\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer -The LGBM classifier in 4 decimal places is 0.9362"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 -Find the feature importance using the optimal ExtraTreesClassifier model. Which features are the most and least important respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection using ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.915, 0.925, 0.921, 0.933, 0.942, 0.93 , 0.938, 0.925, 0.916,\n",
       "       0.921])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ExtraTreesClassifier(min_samples_leaf=2, min_samples_split=8, n_estimators=1000)\n",
    "\n",
    "score = cross_val_score(classifier, X,y, cv=10)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model with x features and y target using the train dataset\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# getting the feature importances using the fitted classifier object\n",
    "feature_Selection = classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Hamoye\\Hamoye_Winter_GroupProject\\hamoye_env\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ExtraTreesClassifier hyparameter = ETC_y_hypar\n",
    "ETC_y_hypar = classifier.predict(Scaler_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13332369, 0.13423436, 0.12741211, 0.13180294, 0.01946212,\n",
       "       0.0218212 , 0.02203949, 0.02166661, 0.09121748, 0.09748929,\n",
       "       0.10482198, 0.09470873])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing feature selection \n",
    "feature_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalising each features \n",
    "normalized_feature_importance = np.std([tree.feature_importances_ for tree in \n",
    "                                        classifier.estimators_], \n",
    "                                        axis = 0)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(X.columns, normalized_feature_importance)\n",
    "plt.xlabel('Feature Labels')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importance Comparison', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer -From the graph, tau1 has the highest importance while p1 has the least importance"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23300995598eec4bcf6bd89cf02d1c3675e8b2616661418dbbf5580aa901878d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
